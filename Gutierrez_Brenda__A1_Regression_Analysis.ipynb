{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af5a117",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<br><h2>A2: Regression Model Development</h2>\n",
    "Brenda Gutierrez\n",
    "<h4>DAT-5390 | Computational Data Analytics with Python</h4>\n",
    "Chase Kusterer - Faculty of Analytics<br>\n",
    "Hult International Business School<br><br><br>\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca739d4",
   "metadata": {},
   "source": [
    "The following code showcases the development of different linear regression models with the objective to predict the revenue generated by Apprentice Chef, a gourmet meal delivery service.\n",
    "\n",
    "Before diving into the model development process, it was crucial to understand the data that we were working with. To begin with, we examined the distribution of the y-variable (i.e. REVENUE) through histograms. Upon inspection, we observed that the original distribution was positively skewed. To tackle this issue, we decided to develop an additional histogram for the logarithmic version of the variable, as it would help us normalize the distribution. Both the original and logarithmic histograms were plotted and analyzed to guide the variable selection process.\n",
    "\n",
    "This initial data exploration and transformation stage is crucial for developing accurate predictive models. The use of the logarithmic version of the REVENUE variable in the subsequent models will allow for a more accurate representation of the data and better predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61ab8ec",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './__datasets/Apprentice_Chef_Dataset_2023.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1072\\1396634121.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# reading the file into Python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mappcustomers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# developing a histogram using HISTPLOT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1374\u001b[0m                 \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m                 ext = inspect_excel_format(\n\u001b[0m\u001b[0;32m   1377\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m                 )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m   1251\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m     ) as handle:\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './__datasets/Apprentice_Chef_Dataset_2023.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data science essentials\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import seaborn as sns # enhanced data visualization\n",
    "\n",
    "\n",
    "# specifying the path and file name\n",
    "file = './__datasets/Apprentice_Chef_Dataset_2023.xlsx'\n",
    "\n",
    "# reading the file into Python\n",
    "appcustomers = pd.read_excel(file)\n",
    "\n",
    "# developing a histogram using HISTPLOT\n",
    "sns.histplot(data   = appcustomers,\n",
    "             x      = 'REVENUE',\n",
    "             kde    = True)\n",
    "\n",
    "\n",
    "# title and axis labels\n",
    "plt.title(label   = \"Original Distribution of Revenue\")\n",
    "plt.xlabel(xlabel = \"Revenue\") # avoiding using dataset labels\n",
    "plt.ylabel(ylabel = \"Count\")\n",
    "\n",
    "# displaying the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c8b612",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "#transforming REVENUE to its logarithmic version\n",
    "appcustomers['log_REVENUE'] = np.log(appcustomers['REVENUE'])\n",
    "\n",
    "# developing a histogram using HISTPLOT\n",
    "sns.histplot(data   = appcustomers,\n",
    "                x      = 'log_REVENUE',\n",
    "                kde    = True)\n",
    "\n",
    "\n",
    "# title and axis labels\n",
    "plt.title(label   = \"Original Distribution of Revenue\")\n",
    "plt.xlabel(xlabel = \"log_Revenue\") # avoiding using dataset labels\n",
    "plt.ylabel(ylabel = \"Count\")\n",
    "\n",
    "# displaying the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbbcac",
   "metadata": {},
   "source": [
    "We can see that the logarithmic version improved the skewness and REVENUE is now closer to a normal distribution, which will improve the performance of the models. For this reason, the latter version of this variable will be used for the development of the models. This pre-processing step is crucial in the development of regression models, as it ensures that the assumptions of linear regression, such as normality, is met.\n",
    "\n",
    "Following up with the exploration of the variables, I made a selection of the x-variables and decided to discard NAME, EMAIL, FIRST NAME, FAMILY NAME since I consider they don't impact the revenue, therefore not relevant for the development of the models. This process of variable selection is important, as it can lead to better-performing models.\n",
    "\n",
    "In order to select the best variables, I decided to find out their percentage of correlation to the y-variable as seen below. This is a good starting point for selecting the best features, as it provides insight into how strongly each variable is related to the target variable. However, it's important to note that correlation does not necessarily imply causation, and other factors may also play a role in predicting the target variable. Therefore, it's necessary to consider other metrics, such as feature importance, when selecting the best features for the model.\n",
    "\n",
    "Overall, these exploratory data analysis steps are critical for building accurate regression models, as they provide insight into the data and enable the selection of the best features. With the selected features and pre-processed target variable, we can now move on to building and tuning the regression models to predict the revenue of Apprentice Chef.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of continuous features (including REVENUE)\n",
    "variables = ['REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH', 'CONTACTS_W_CUSTOMER_SERVICE', \n",
    "         'PRODUCT_CATEGORIES_VIEWED', 'AVG_TIME_PER_SITE_VISIT', 'CANCELLATIONS_AFTER_NOON', \n",
    "         'PC_LOGINS', 'MOBILE_LOGINS', 'WEEKLY_PLAN', 'LATE_DELIVERIES ', 'AVG_PREP_VID_TIME', \n",
    "         'AVG_MEAN_RATING', 'LARGEST_ORDER_SIZE', 'TOTAL_PHOTOS_VIEWED']\n",
    "\n",
    "\n",
    "# developing a correlation matrix based on continuous features\n",
    "appcustomers_corr = appcustomers[variables].corr(method = 'pearson')\n",
    "\n",
    "\n",
    "# filtering the results to only show correlations with REVENUE\n",
    "appcustomers_corr.loc[ : ,'REVENUE'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c52ba",
   "metadata": {},
   "source": [
    "In this case, it seems that most of the variables have a good correlation with REVENUE, meaning that they have a strong relationship with the target variable. Based on this observation, the decision was made to include all of the variables in the model, as they are likely to have a strong positive relationship with REVENUE.\n",
    "\n",
    "Including all of the variables in the model may improve the model's ability to predict the target variable, as it captures more information about the data.\n",
    "\n",
    "After running some models with those variables, I decided to transform TOTAL MEALS ORDERED to its logarithmic version to see if that improved the final scores and it did but I decided to take it further by performing feature engineering of that same variable with UNIQUE MEALS PURCHASED. By creating this new feature, we can capture information about customers who purchase a larger proportion of unique meals relative to the total number of meals they order. This may be valuable information to have because it could indicate that these customers are more likely to be interested in new or unique dishes that the company may offer, and therefore may be more valuable customers to target for promotions or special offers.\n",
    "\n",
    "Additionally, this new feature could help capture more information about the customer's overall spending habits, which could be useful in predicting their future spending behavior. For example, if a customer has a high value of AVG MEALS, it may indicate that they are willing to spend more money to try new and unique dishes, which could be an important factor to consider when predicting their future revenue potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf # regression modeling\n",
    "from sklearn.model_selection import train_test_split # train/test split\n",
    "import sklearn.linear_model # linear modeling in scikit-learn\n",
    "from sklearn.tree     import DecisionTreeRegressor     # regression trees\n",
    "from sklearn.ensemble import RandomForestRegressor     # random forest\n",
    "from sklearn.ensemble import GradientBoostingRegressor # gbm\n",
    "\n",
    "appcustomers['REVENUE'] = np.log(appcustomers['REVENUE'])\n",
    "\n",
    "#transforming REVENUE to its logarithmic version\n",
    "appcustomers['TOTAL_MEALS_ORDERED'] = np.log(appcustomers['TOTAL_MEALS_ORDERED'])\n",
    "\n",
    "#feature engineering in order to improve score\n",
    "\n",
    "appcustomers['AVG_MEALS'] = appcustomers['UNIQUE_MEALS_PURCH'] / appcustomers['TOTAL_MEALS_ORDERED']\n",
    "\n",
    "#including new variable into x-variables\n",
    "\n",
    "x_var = ['AVG_MEALS' , 'TOTAL_MEALS_ORDERED' , 'UNIQUE_MEALS_PURCH' , 'CONTACTS_W_CUSTOMER_SERVICE' , \n",
    "         'PRODUCT_CATEGORIES_VIEWED' , 'AVG_TIME_PER_SITE_VISIT' , 'CANCELLATIONS_AFTER_NOON' , \n",
    "         'PC_LOGINS' , 'MOBILE_LOGINS' , 'WEEKLY_PLAN' , 'LATE_DELIVERIES ' , 'AVG_PREP_VID_TIME' , \n",
    "         'AVG_MEAN_RATING' , 'LARGEST_ORDER_SIZE' , 'TOTAL_PHOTOS_VIEWED']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377838f",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "After finalizing the features to be included in the model, the next step was to train and evaluate its performance. To accomplish this, we used a technique called data splitting. Data splitting is a common technique in machine learning where the available data is divided into two sets: a training set and a testing set.\n",
    "\n",
    "We used a ratio of 75:25 for training and testing, respectively. This means that 75% of the available data was used for training the model, while the remaining 25% was set aside for testing the model's performance. This is a common ratio used in data science because it provides a good balance between having enough data to train the model effectively while also having enough data to test the model's performance accurately.\n",
    "\n",
    "By splitting the data into a training and testing set, we were able to evaluate how well the model generalizes to new data. The training set was used to fit the model, while the testing set was used to evaluate its performance. This allowed us to assess the model's ability to make accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing for scikit-learn\n",
    "\n",
    "# preparing x-variables for training and testing\n",
    "x_data = appcustomers[x_var]\n",
    "\n",
    "\n",
    "# preparing response variable\n",
    "y_data      = appcustomers['REVENUE']\n",
    "\n",
    "\n",
    "###############################################\n",
    "## setting up train-test split ##\n",
    "###############################################\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "                                                     x_data, # x-variables \n",
    "                                                     y_data, # y-variable  \n",
    "                                                     test_size    = 0.25,\n",
    "                                                     random_state = 219)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e8b4a",
   "metadata": {},
   "source": [
    "After the data exploration and selection of the most relevant variables, the next step was to feed the data into the different linear regression models. The objective of this process was to analyze the relationships between the various features of the data and the target variable, and to determine the model that would provide the best fit for the data.\n",
    "\n",
    "By fitting the data into the models, I was able to evaluate their performance and identify any potential areas of improvement or weaknesses. This allowed me to refine the models, by modifying their hyperparameters, and test their performance again. Through this iterative process, I was able to select the model that provided the best results for my specific use case, which in this case was predicting the revenue generated by Apprentice Chef.\n",
    "\n",
    "The final model selection was made taking into consideration not only the accuracy scores of the models but also the train-test gap, which is a measure of overfitting. This ensures that the model generalizes well to unseen data and is not just fitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723521ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CANDIDATE MODELS\n",
    "\n",
    "#Defining function for Linear Regression model\n",
    "\n",
    "def linear_regression(): \n",
    "\n",
    "    # Setting a model name\n",
    "    model_name = \"Linear Regression\"\n",
    "\n",
    "\n",
    "    # INSTANTIATING a model object \n",
    "    model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "    # FITTING to the training data\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING on new data\n",
    "    model_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # SCORING the results\n",
    "    model_train_score = model.score(x_train, y_train).round(4) # using R-square\n",
    "    model_test_score  = model.score(x_test, y_test).round(4)   # using R-square\n",
    "    model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "    # displaying results\n",
    "    model_name =  f\"\"\"\\\n",
    "    Model Name:     {model_name}\n",
    "    Train_Score:    {model_train_score}\n",
    "    Test_Score:     {model_test_score}\n",
    "    Train-Test Gap: {model_gap}\n",
    "    \"\"\"\n",
    "\n",
    "    print(model_name) #printing results\n",
    "    \n",
    "\n",
    "#Defining function for Lasso Regression model\n",
    "\n",
    "def lasso_regression():\n",
    "    \n",
    "    # Setting a model name\n",
    "    model_name = \"Lasso Regression\"\n",
    "\n",
    "\n",
    "    # INSTANTIATING a model object \n",
    "    model = sklearn.linear_model.Lasso()\n",
    "\n",
    "\n",
    "    # FITTING to the training data\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING on new data\n",
    "    model_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # SCORING the results\n",
    "    model_train_score = model.score(x_train, y_train).round(4) # using R-square\n",
    "    model_test_score  = model.score(x_test, y_test).round(4)   # using R-square\n",
    "    model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "    # displaying results\n",
    "    model_name =  f\"\"\"\\\n",
    "    Model Name:     {model_name}\n",
    "    Train_Score:    {model_train_score}\n",
    "    Test_Score:     {model_test_score}\n",
    "    Train-Test Gap: {model_gap}\n",
    "    \"\"\"\n",
    "\n",
    "    print(model_name) #printing results\n",
    "    \n",
    "\n",
    "#Defining function for ARD Regression model\n",
    "    \n",
    "def ard_regression():\n",
    "    \n",
    "    # Setting a model name\n",
    "    model_name = \"ARD Regression\"\n",
    "\n",
    "\n",
    "    # INSTANTIATING a model object \n",
    "    model = sklearn.linear_model.ARDRegression()\n",
    "\n",
    "\n",
    "    # FITTING to the training data\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING on new data\n",
    "    model_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # SCORING the results\n",
    "    model_train_score = model.score(x_train, y_train).round(4) # using R-square\n",
    "    model_test_score  = model.score(x_test, y_test).round(4)   # using R-square\n",
    "    model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "    # displaying results\n",
    "    model_name =  f\"\"\"\\\n",
    "    Model Name:     {model_name}\n",
    "    Train_Score:    {model_train_score}\n",
    "    Test_Score:     {model_test_score}\n",
    "    Train-Test Gap: {model_gap}\n",
    "    \"\"\"\n",
    "\n",
    "    print(model_name) #printing results\n",
    "    \n",
    "\n",
    "#Defining function for Unpruned Regression Tree model\n",
    "\n",
    "def unpruned_regression():\n",
    "    \n",
    "    model_name = 'Unpruned Regression Tree'\n",
    "\n",
    "    # INSTANTIATING a model object \n",
    "    model = DecisionTreeRegressor()\n",
    "\n",
    "\n",
    "    # FITTING to the training data\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING on new data\n",
    "    model_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # SCORING the results\n",
    "    model_train_score = model.score(x_train, y_train).round(4) # using R-square\n",
    "    model_test_score  = model.score(x_test, y_test).round(4)   # using R-square\n",
    "    model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "    # displaying results\n",
    "    model_name =  f\"\"\"\\\n",
    "    Model Name:     {model_name}\n",
    "    Train_Score:    {model_train_score}\n",
    "    Test_Score:     {model_test_score}\n",
    "    Train-Test Gap: {model_gap}\n",
    "    \"\"\"\n",
    "\n",
    "    print(model_name) #printing results\n",
    "    \n",
    "\n",
    "#Defining function for Pruned Regression Tree model\n",
    "    \n",
    "def pruned_regression():\n",
    "    \n",
    "    model_name = 'Pruned Regression Tree'\n",
    "\n",
    "    # INSTANTIATING a model object \n",
    "    model = DecisionTreeRegressor(max_depth         = 5, #hyperparameter tuning\n",
    "                                  min_samples_split = 35, #hyperparameter tuning\n",
    "                                  random_state      = 219) #hyperparameter tuning\n",
    "\n",
    "    # FITTING to the training data\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING on new data\n",
    "    model_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # SCORING the results\n",
    "    model_train_score = model.score(x_train, y_train).round(4) # using R-square\n",
    "    model_test_score  = model.score(x_test, y_test).round(4)   # using R-square\n",
    "    model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "    # displaying results\n",
    "    model_name =  f\"\"\"\\\n",
    "    Model Name:     {model_name}\n",
    "    Train_Score:    {model_train_score}\n",
    "    Test_Score:     {model_test_score}\n",
    "    Train-Test Gap: {model_gap}\n",
    "    \"\"\"\n",
    "\n",
    "    print(model_name) #printing results\n",
    "    \n",
    "\n",
    "#Defining function for Unpruned Random Forest model\n",
    "\n",
    "def unpruned_random():\n",
    "    \n",
    "    # specifying a model name\n",
    "    model_name = 'Unpruned Random Forest'\n",
    "\n",
    "\n",
    "    # INSTANTIATING\n",
    "    model = RandomForestRegressor(n_estimators     = 100, #hyperparameter tuning\n",
    "                                  criterion        = 'squared_error', #hyperparameter tuning\n",
    "                                  max_depth        = None, #hyperparameter tuning\n",
    "                                  min_samples_leaf = 1, #hyperparameter tuning\n",
    "                                  bootstrap        = True, #hyperparameter tuning\n",
    "                                  warm_start       = False, #hyperparameter tuning\n",
    "                                  random_state     = 219) #hyperparameter tuning\n",
    "\n",
    "    # FITTING the training data\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING based on the testing set\n",
    "    model_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # SCORING the results\n",
    "    model_train_score = model.score(x_train, y_train).round(4) # using R-square\n",
    "    model_test_score  = model.score(x_test, y_test).round(4)   # using R-square\n",
    "    model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "    # displaying results\n",
    "    model_name =  f\"\"\"\\\n",
    "    Model Name:     {model_name}\n",
    "    Train_Score:    {model_train_score}\n",
    "    Test_Score:     {model_test_score}\n",
    "    Train-Test Gap: {model_gap}\n",
    "    \"\"\"\n",
    "\n",
    "    print(model_name) #printing results\n",
    "    \n",
    "    \n",
    "#Defining function for Pruned Random Forest model\n",
    "\n",
    "def pruned_random():\n",
    "    \n",
    "    model_name = 'Pruned Random Forest'\n",
    "\n",
    "\n",
    "    # INSTANTIATING a random forest model with default values\n",
    "    model = RandomForestRegressor(n_estimators     = 200, #hyperparameter tuning\n",
    "                                  criterion        = 'squared_error', #hyperparameter tuning\n",
    "                                  max_depth        = 200, #hyperparameter tuning\n",
    "                                  min_samples_leaf = 14, #hyperparameter tuning\n",
    "                                  bootstrap        = True, #hyperparameter tuning\n",
    "                                  warm_start       = False, #hyperparameter tuning\n",
    "                                  random_state     = 219) #hyperparameter tuning\n",
    "\n",
    "\n",
    "    # FITTING the training data\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING based on the testing set\n",
    "    model_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # SCORING the results\n",
    "    model_train_score = model.score(x_train, y_train).round(4) # using R-square\n",
    "    model_test_score  = model.score(x_test, y_test).round(4)   # using R-square\n",
    "    model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "    # displaying results\n",
    "    model_name =  f\"\"\"\\\n",
    "    Model Name:     {model_name}\n",
    "    Train_Score:    {model_train_score}\n",
    "    Test_Score:     {model_test_score}\n",
    "    Train-Test Gap: {model_gap}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(model_name) #printing results\n",
    "\n",
    "#FINAL MODEL\n",
    "\n",
    "#Defining function for Unpruned Gradient Boosted Machines model\n",
    "\n",
    "def unpruned_gbm():\n",
    "    \n",
    "    # specifying a model name\n",
    "    model_name = 'Unpruned Gradient Boosted Machines'\n",
    "\n",
    "\n",
    "    # INSTANTIATING the model object\n",
    "    model = GradientBoostingRegressor(loss          = 'huber', #hyperparameter tuning\n",
    "                                      learning_rate = 0.1, #hyperparameter tuning\n",
    "                                      n_estimators  = 200, #hyperparameter tuning\n",
    "                                      criterion     = 'friedman_mse', #hyperparameter tuning\n",
    "                                      max_depth     = 2, #hyperparameter tuning\n",
    "                                      warm_start    = False,\n",
    "                                      random_state  = 219) #hyperparameter tuning\n",
    "\n",
    "\n",
    "    # FITTING the training data\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING based on the testing set\n",
    "    model_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # SCORING the results\n",
    "    model_train_score = model.score(x_train, y_train).round(4) # using R-square\n",
    "    model_test_score  = model.score(x_test, y_test).round(4)   # using R-square\n",
    "    model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "    # displaying results\n",
    "    model_name =  f\"\"\"\\\n",
    "    \n",
    "            BEST MODEL\n",
    "    \n",
    "    Model Name:     {model_name}\n",
    "    Train_Score:    {model_train_score}\n",
    "    Test_Score:     {model_test_score}\n",
    "    Train-Test Gap: {model_gap}\n",
    "    \"\"\"\n",
    "\n",
    "    print(model_name) #printing results\n",
    "\n",
    "    \n",
    "#Calling functions in order to print them as an output\n",
    "\n",
    "linear_regression()\n",
    "\n",
    "lasso_regression()\n",
    "\n",
    "ard_regression()\n",
    "\n",
    "unpruned_regression()\n",
    "\n",
    "pruned_regression()\n",
    "\n",
    "unpruned_random()\n",
    "\n",
    "pruned_random()\n",
    "\n",
    "unpruned_gbm()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436e061",
   "metadata": {},
   "source": [
    "The unpruned GBM has the highest test score (0.8139) out of all the models, which means that it is the best at predicting the revenue generated by Apprentice Chef. The use of logarithmic versions of the REVENUE and TOTAL MEALS ORDERED variables as well as for the hyperparameter tuning of the GBM allowed for the model to fit more complex data relationships and better handle outliers, resulting in improved performance.\n",
    "\n",
    "After training and testing the model, I found that it had a training score of 0.8602 and a test score of 0.8139. The high training score indicates that the model fits the training data well, while the high test score suggests that it can generalize well to new data. \n",
    "\n",
    "It's important to remember that a large train-test gap could indicate overfitting, meaning the model is memorizing the training data and not generalizing well to new, unseen data. In contrast, a small train-test gap suggests that the model is generalizing well to new data, and is not just memorizing the training data.\n",
    "\n",
    "In this case, the unpruned GBM model has the smallest train-test gap of 0.0463 out of the other models (even the pruned ones), which suggests that it is generalizing well to new data. This, along with its high test score, makes the unpruned GBM the best model for predicting the revenue generated by Apprentice Chef, enabling the company to make data-driven business decisions based on the insights provided by the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
